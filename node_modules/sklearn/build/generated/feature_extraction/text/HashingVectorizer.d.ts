import { PythonBridge, SparseMatrix } from '@/sklearn/types';
/**
  Convert a collection of text documents to a matrix of token occurrences.

  It turns a collection of text documents into a scipy.sparse matrix holding token occurrence counts (or binary occurrence information), possibly normalized as token frequencies if norm=’l1’ or projected on the euclidean unit sphere if norm=’l2’.

  This text vectorizer implementation uses the hashing trick to find the token string name to feature integer index mapping.

  This strategy has several advantages:

  @see https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html
 */
export declare class HashingVectorizer {
    id: string;
    opts: any;
    _py: PythonBridge;
    _isInitialized: boolean;
    _isDisposed: boolean;
    constructor(opts?: HashingVectorizerOptions);
    get py(): PythonBridge;
    set py(pythonBridge: PythonBridge);
    /**
      Initializes the underlying Python resources.
  
      This instance is not usable until the `Promise` returned by `init()` resolves.
     */
    init(py: PythonBridge): Promise<void>;
    /**
      Disposes of the underlying Python resources.
  
      Once `dispose()` is called, the instance is no longer usable.
     */
    dispose(): Promise<void>;
    /**
      Return a callable to process input data.
  
      The callable handles preprocessing, tokenization, and n-grams generation.
     */
    build_analyzer(opts: HashingVectorizerBuildAnalyzerOptions): Promise<any>;
    /**
      Return a function to preprocess the text before tokenization.
     */
    build_preprocessor(opts: HashingVectorizerBuildPreprocessorOptions): Promise<any>;
    /**
      Return a function that splits a string into a sequence of tokens.
     */
    build_tokenizer(opts: HashingVectorizerBuildTokenizerOptions): Promise<any>;
    /**
      Decode the input into a string of unicode symbols.
  
      The decoding strategy depends on the vectorizer parameters.
     */
    decode(opts: HashingVectorizerDecodeOptions): Promise<any>;
    /**
      Only validates estimator’s parameters.
  
      This method allows to: (i) validate the estimator’s parameters and (ii) be consistent with the scikit-learn transformer API.
     */
    fit(opts: HashingVectorizerFitOptions): Promise<any>;
    /**
      Transform a sequence of documents to a document-term matrix.
     */
    fit_transform(opts: HashingVectorizerFitTransformOptions): Promise<SparseMatrix[]>;
    /**
      Build or fetch the effective stop words list.
     */
    get_stop_words(opts: HashingVectorizerGetStopWordsOptions): Promise<any>;
    /**
      Only validates estimator’s parameters.
  
      This method allows to: (i) validate the estimator’s parameters and (ii) be consistent with the scikit-learn transformer API.
     */
    partial_fit(opts: HashingVectorizerPartialFitOptions): Promise<any>;
    /**
      Set output container.
  
      See Introducing the set_output API for an example on how to use the API.
     */
    set_output(opts: HashingVectorizerSetOutputOptions): Promise<any>;
    /**
      Transform a sequence of documents to a document-term matrix.
     */
    transform(opts: HashingVectorizerTransformOptions): Promise<SparseMatrix[]>;
}
export interface HashingVectorizerOptions {
    /**
      If 'filename', the sequence passed as an argument to fit is expected to be a list of filenames that need reading to fetch the raw content to analyze.
  
      @defaultValue `'content'`
     */
    input?: 'filename' | 'file' | 'content';
    /**
      If bytes or files are given to analyze, this encoding is used to decode.
  
      @defaultValue `'utf-8'`
     */
    encoding?: string;
    /**
      Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given encoding. By default, it is ‘strict’, meaning that a UnicodeDecodeError will be raised. Other values are ‘ignore’ and ‘replace’.
  
      @defaultValue `'strict'`
     */
    decode_error?: 'strict' | 'ignore' | 'replace';
    /**
      Remove accents and perform other character normalization during the preprocessing step. ‘ascii’ is a fast method that only works on characters that have a direct ASCII mapping. ‘unicode’ is a slightly slower method that works on any character. None (default) does nothing.
  
      Both ‘ascii’ and ‘unicode’ use NFKD normalization from unicodedata.normalize.
     */
    strip_accents?: 'ascii' | 'unicode';
    /**
      Convert all characters to lowercase before tokenizing.
  
      @defaultValue `true`
     */
    lowercase?: boolean;
    /**
      Override the preprocessing (string transformation) stage while preserving the tokenizing and n-grams generation steps. Only applies if analyzer is not callable.
     */
    preprocessor?: any;
    /**
      Override the string tokenization step while preserving the preprocessing and n-grams generation steps. Only applies if analyzer == 'word'.
     */
    tokenizer?: any;
    /**
      If ‘english’, a built-in stop word list for English is used. There are several known issues with ‘english’ and you should consider an alternative (see Using stop words).
  
      If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens. Only applies if analyzer == 'word'.
     */
    stop_words?: 'english' | any[];
    /**
      Regular expression denoting what constitutes a “token”, only used if analyzer == 'word'. The default regexp selects tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator).
  
      If there is a capturing group in token_pattern then the captured group content, not the entire match, becomes the token. At most one capturing group is permitted.
     */
    token_pattern?: string;
    /**
      The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used. For example an ngram_range of (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams. Only applies if analyzer is not callable.
     */
    ngram_range?: any;
    /**
      Whether the feature should be made of word or character n-grams. Option ‘char_wb’ creates character n-grams only from text inside word boundaries; n-grams at the edges of words are padded with space.
  
      If a callable is passed it is used to extract the sequence of features out of the raw, unprocessed input.
  
      @defaultValue `'word'`
     */
    analyzer?: 'word' | 'char' | 'char_wb';
    /**
      The number of features (columns) in the output matrices. Small numbers of features are likely to cause hash collisions, but large numbers will cause larger coefficient dimensions in linear learners.
     */
    n_features?: number;
    /**
      If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts.
  
      @defaultValue `false`
     */
    binary?: boolean;
    /**
      Norm used to normalize term vectors. None for no normalization.
  
      @defaultValue `'l2'`
     */
    norm?: 'l1' | 'l2';
    /**
      When True, an alternating sign is added to the features as to approximately conserve the inner product in the hashed space even for small n_features. This approach is similar to sparse random projection.
  
      @defaultValue `true`
     */
    alternate_sign?: boolean;
    /**
      Type of the matrix returned by fit_transform() or transform().
     */
    dtype?: any;
}
export interface HashingVectorizerBuildAnalyzerOptions {
}
export interface HashingVectorizerBuildPreprocessorOptions {
}
export interface HashingVectorizerBuildTokenizerOptions {
}
export interface HashingVectorizerDecodeOptions {
    /**
      The string to decode.
     */
    doc?: string;
}
export interface HashingVectorizerFitOptions {
    /**
      Training data.
     */
    X?: any;
    /**
      Not used, present for API consistency by convention.
     */
    y?: any;
}
export interface HashingVectorizerFitTransformOptions {
    /**
      Samples. Each sample must be a text document (either bytes or unicode strings, file name or file object depending on the constructor argument) which will be tokenized and hashed.
     */
    X?: any;
    /**
      Ignored. This parameter exists only for compatibility with sklearn.pipeline.Pipeline.
     */
    y?: any;
}
export interface HashingVectorizerGetStopWordsOptions {
}
export interface HashingVectorizerPartialFitOptions {
    /**
      Training data.
     */
    X?: any;
    /**
      Not used, present for API consistency by convention.
     */
    y?: any;
}
export interface HashingVectorizerSetOutputOptions {
    /**
      Configure output of transform and fit_transform.
     */
    transform?: 'default' | 'pandas';
}
export interface HashingVectorizerTransformOptions {
    /**
      Samples. Each sample must be a text document (either bytes or unicode strings, file name or file object depending on the constructor argument) which will be tokenized and hashed.
     */
    X?: any;
}
//# sourceMappingURL=HashingVectorizer.d.ts.map