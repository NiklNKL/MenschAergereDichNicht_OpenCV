import { PythonBridge, NDArray, ArrayLike, SparseMatrix } from '@/sklearn/types';
/**
  Latent Dirichlet Allocation with online variational Bayes algorithm.

  The implementation is based on [1] and [2].

  @see https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html
 */
export declare class LatentDirichletAllocation {
    id: string;
    opts: any;
    _py: PythonBridge;
    _isInitialized: boolean;
    _isDisposed: boolean;
    constructor(opts?: LatentDirichletAllocationOptions);
    get py(): PythonBridge;
    set py(pythonBridge: PythonBridge);
    /**
      Initializes the underlying Python resources.
  
      This instance is not usable until the `Promise` returned by `init()` resolves.
     */
    init(py: PythonBridge): Promise<void>;
    /**
      Disposes of the underlying Python resources.
  
      Once `dispose()` is called, the instance is no longer usable.
     */
    dispose(): Promise<void>;
    /**
      Learn model for the data X with variational Bayes method.
  
      When learning_method is ‘online’, use mini-batch update. Otherwise, use batch update.
     */
    fit(opts: LatentDirichletAllocationFitOptions): Promise<any>;
    /**
      Fit to data, then transform it.
  
      Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.
     */
    fit_transform(opts: LatentDirichletAllocationFitTransformOptions): Promise<any[]>;
    /**
      Get output feature names for transformation.
  
      The feature names out will prefixed by the lowercased class name. For example, if the transformer outputs 3 features, then the feature names out are: ["class_name0", "class_name1", "class_name2"].
     */
    get_feature_names_out(opts: LatentDirichletAllocationGetFeatureNamesOutOptions): Promise<any>;
    /**
      Online VB with Mini-Batch update.
     */
    partial_fit(opts: LatentDirichletAllocationPartialFitOptions): Promise<any>;
    /**
      Calculate approximate perplexity for data X.
  
      Perplexity is defined as exp(-1. * log-likelihood per word)
     */
    perplexity(opts: LatentDirichletAllocationPerplexityOptions): Promise<number>;
    /**
      Calculate approximate log-likelihood as score.
     */
    score(opts: LatentDirichletAllocationScoreOptions): Promise<number>;
    /**
      Set output container.
  
      See Introducing the set_output API for an example on how to use the API.
     */
    set_output(opts: LatentDirichletAllocationSetOutputOptions): Promise<any>;
    /**
      Transform data X according to the fitted model.
     */
    transform(opts: LatentDirichletAllocationTransformOptions): Promise<NDArray[]>;
    /**
      Variational parameters for topic word distribution. Since the complete conditional for topic word distribution is a Dirichlet, components_[i, j] can be viewed as pseudocount that represents the number of times word j was assigned to topic i. It can also be viewed as distribution over the words for each topic after normalization: model.components_ / model.components_.sum(axis=1)[:, np.newaxis].
     */
    get components_(): Promise<NDArray[]>;
    /**
      Exponential value of expectation of log topic word distribution. In the literature, this is exp(E[log(beta)]).
     */
    get exp_dirichlet_component_(): Promise<NDArray[]>;
    /**
      Number of iterations of the EM step.
     */
    get n_batch_iter_(): Promise<number>;
    /**
      Number of features seen during fit.
     */
    get n_features_in_(): Promise<number>;
    /**
      Names of features seen during fit. Defined only when X has feature names that are all strings.
     */
    get feature_names_in_(): Promise<NDArray>;
    /**
      Number of passes over the dataset.
     */
    get n_iter_(): Promise<number>;
    /**
      Final perplexity score on training set.
     */
    get bound_(): Promise<number>;
    /**
      Prior of document topic distribution theta. If the value is None, it is 1 / n_components.
     */
    get doc_topic_prior_(): Promise<number>;
    /**
      RandomState instance that is generated either from a seed, the random number generator or by np.random.
     */
    get random_state_(): Promise<any>;
    /**
      Prior of topic word distribution beta. If the value is None, it is 1 / n_components.
     */
    get topic_word_prior_(): Promise<number>;
}
export interface LatentDirichletAllocationOptions {
    /**
      Number of topics.
  
      @defaultValue `10`
     */
    n_components?: number;
    /**
      Prior of document topic distribution theta. If the value is None, defaults to 1 / n_components. In [1], this is called alpha.
     */
    doc_topic_prior?: number;
    /**
      Prior of topic word distribution beta. If the value is None, defaults to 1 / n_components. In [1], this is called eta.
     */
    topic_word_prior?: number;
    /**
      Method used to update _component. Only used in fit method. In general, if the data size is large, the online update will be much faster than the batch update.
  
      Valid options:
  
      @defaultValue `'batch'`
     */
    learning_method?: 'batch' | 'online';
    /**
      It is a parameter that control learning rate in the online learning method. The value should be set between (0.5, 1.0] to guarantee asymptotic convergence. When the value is 0.0 and batch_size is n_samples, the update method is same as batch learning. In the literature, this is called kappa.
  
      @defaultValue `0.7`
     */
    learning_decay?: number;
    /**
      A (positive) parameter that downweights early iterations in online learning.  It should be greater than 1.0. In the literature, this is called tau_0.
  
      @defaultValue `10`
     */
    learning_offset?: number;
    /**
      The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the fit method, and not the partial_fit method.
  
      @defaultValue `10`
     */
    max_iter?: number;
    /**
      Number of documents to use in each EM iteration. Only used in online learning.
  
      @defaultValue `128`
     */
    batch_size?: number;
    /**
      How often to evaluate perplexity. Only used in fit method. set it to 0 or negative number to not evaluate perplexity in training at all. Evaluating perplexity can help you check convergence in training process, but it will also increase total training time. Evaluating perplexity in every iteration might increase training time up to two-fold.
  
      @defaultValue `-1`
     */
    evaluate_every?: number;
    /**
      Total number of documents. Only used in the partial_fit method.
  
      @defaultValue `1000000`
     */
    total_samples?: number;
    /**
      Perplexity tolerance in batch learning. Only used when evaluate_every is greater than 0.
  
      @defaultValue `0.1`
     */
    perp_tol?: number;
    /**
      Stopping tolerance for updating document topic distribution in E-step.
  
      @defaultValue `0.001`
     */
    mean_change_tol?: number;
    /**
      Max number of iterations for updating document topic distribution in the E-step.
  
      @defaultValue `100`
     */
    max_doc_update_iter?: number;
    /**
      The number of jobs to use in the E-step. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details.
     */
    n_jobs?: number;
    /**
      Verbosity level.
  
      @defaultValue `0`
     */
    verbose?: number;
    /**
      Pass an int for reproducible results across multiple function calls. See Glossary.
     */
    random_state?: number;
}
export interface LatentDirichletAllocationFitOptions {
    /**
      Document word matrix.
     */
    X?: ArrayLike | SparseMatrix[];
    /**
      Not used, present here for API consistency by convention.
     */
    y?: any;
}
export interface LatentDirichletAllocationFitTransformOptions {
    /**
      Input samples.
     */
    X?: ArrayLike[];
    /**
      Target values (None for unsupervised transformations).
     */
    y?: ArrayLike;
    /**
      Additional fit parameters.
     */
    fit_params?: any;
}
export interface LatentDirichletAllocationGetFeatureNamesOutOptions {
    /**
      Only used to validate feature names with the names seen in fit.
     */
    input_features?: any;
}
export interface LatentDirichletAllocationPartialFitOptions {
    /**
      Document word matrix.
     */
    X?: ArrayLike | SparseMatrix[];
    /**
      Not used, present here for API consistency by convention.
     */
    y?: any;
}
export interface LatentDirichletAllocationPerplexityOptions {
    /**
      Document word matrix.
     */
    X?: ArrayLike | SparseMatrix[];
    /**
      Do sub-sampling or not.
     */
    sub_sampling?: boolean;
}
export interface LatentDirichletAllocationScoreOptions {
    /**
      Document word matrix.
     */
    X?: ArrayLike | SparseMatrix[];
    /**
      Not used, present here for API consistency by convention.
     */
    y?: any;
}
export interface LatentDirichletAllocationSetOutputOptions {
    /**
      Configure output of transform and fit_transform.
     */
    transform?: 'default' | 'pandas';
}
export interface LatentDirichletAllocationTransformOptions {
    /**
      Document word matrix.
     */
    X?: ArrayLike | SparseMatrix[];
}
//# sourceMappingURL=LatentDirichletAllocation.d.ts.map