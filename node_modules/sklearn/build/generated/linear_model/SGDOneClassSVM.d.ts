import { PythonBridge, NDArray, ArrayLike, SparseMatrix } from '@/sklearn/types';
/**
  Solves linear One-Class SVM using Stochastic Gradient Descent.

  This implementation is meant to be used with a kernel approximation technique (e.g. sklearn.kernel_approximation.Nystroem) to obtain results similar to sklearn.svm.OneClassSVM which uses a Gaussian kernel by default.

  @see https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDOneClassSVM.html
 */
export declare class SGDOneClassSVM {
    id: string;
    opts: any;
    _py: PythonBridge;
    _isInitialized: boolean;
    _isDisposed: boolean;
    constructor(opts?: SGDOneClassSVMOptions);
    get py(): PythonBridge;
    set py(pythonBridge: PythonBridge);
    /**
      Initializes the underlying Python resources.
  
      This instance is not usable until the `Promise` returned by `init()` resolves.
     */
    init(py: PythonBridge): Promise<void>;
    /**
      Disposes of the underlying Python resources.
  
      Once `dispose()` is called, the instance is no longer usable.
     */
    dispose(): Promise<void>;
    /**
      Signed distance to the separating hyperplane.
  
      Signed distance is positive for an inlier and negative for an outlier.
     */
    decision_function(opts: SGDOneClassSVMDecisionFunctionOptions): Promise<ArrayLike>;
    /**
      Convert coefficient matrix to dense array format.
  
      Converts the coef_ member (back) to a numpy.ndarray. This is the default format of coef_ and is required for fitting, so calling this method is only required on models that have previously been sparsified; otherwise, it is a no-op.
     */
    densify(opts: SGDOneClassSVMDensifyOptions): Promise<any>;
    /**
      Fit linear One-Class SVM with Stochastic Gradient Descent.
  
      This solves an equivalent optimization problem of the One-Class SVM primal optimization problem and returns a weight vector w and an offset rho such that the decision function is given by <w, x> - rho.
     */
    fit(opts: SGDOneClassSVMFitOptions): Promise<any>;
    /**
      Perform fit on X and returns labels for X.
  
      Returns -1 for outliers and 1 for inliers.
     */
    fit_predict(opts: SGDOneClassSVMFitPredictOptions): Promise<NDArray>;
    /**
      Fit linear One-Class SVM with Stochastic Gradient Descent.
     */
    partial_fit(opts: SGDOneClassSVMPartialFitOptions): Promise<any>;
    /**
      Return labels (1 inlier, -1 outlier) of the samples.
     */
    predict(opts: SGDOneClassSVMPredictOptions): Promise<any>;
    /**
      Raw scoring function of the samples.
     */
    score_samples(opts: SGDOneClassSVMScoreSamplesOptions): Promise<ArrayLike>;
    /**
      Convert coefficient matrix to sparse format.
  
      Converts the coef_ member to a scipy.sparse matrix, which for L1-regularized models can be much more memory- and storage-efficient than the usual numpy.ndarray representation.
  
      The intercept_ member is not converted.
     */
    sparsify(opts: SGDOneClassSVMSparsifyOptions): Promise<any>;
    /**
      Weights assigned to the features.
     */
    get coef_(): Promise<NDArray[]>;
    /**
      Offset used to define the decision function from the raw scores. We have the relation: decision_function = score_samples - offset.
     */
    get offset_(): Promise<NDArray>;
    /**
      The actual number of iterations to reach the stopping criterion.
     */
    get n_iter_(): Promise<number>;
    /**
      Number of weight updates performed during training. Same as (n_iter_ * n_samples + 1).
     */
    get t_(): Promise<number>;
    get loss_function_(): Promise<any>;
    /**
      Number of features seen during fit.
     */
    get n_features_in_(): Promise<number>;
    /**
      Names of features seen during fit. Defined only when X has feature names that are all strings.
     */
    get feature_names_in_(): Promise<NDArray>;
}
export interface SGDOneClassSVMOptions {
    /**
      The nu parameter of the One Class SVM: an upper bound on the fraction of training errors and a lower bound of the fraction of support vectors. Should be in the interval (0, 1]. By default 0.5 will be taken.
  
      @defaultValue `0.5`
     */
    nu?: number;
    /**
      Whether the intercept should be estimated or not. Defaults to True.
  
      @defaultValue `true`
     */
    fit_intercept?: boolean;
    /**
      The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the fit method, and not the partial_fit. Defaults to 1000.
  
      @defaultValue `1000`
     */
    max_iter?: number;
    /**
      The stopping criterion. If it is not None, the iterations will stop when (loss > previous_loss - tol). Defaults to 1e-3.
  
      @defaultValue `0.001`
     */
    tol?: number;
    /**
      Whether or not the training data should be shuffled after each epoch. Defaults to True.
  
      @defaultValue `true`
     */
    shuffle?: boolean;
    /**
      The verbosity level.
  
      @defaultValue `0`
     */
    verbose?: number;
    /**
      The seed of the pseudo random number generator to use when shuffling the data.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.
     */
    random_state?: number;
    /**
      The learning rate schedule to use with fit. (If using partial_fit, learning rate must be controlled directly).
  
      @defaultValue `'optimal'`
     */
    learning_rate?: 'constant' | 'optimal' | 'invscaling' | 'adaptive';
    /**
      The initial learning rate for the ‘constant’, ‘invscaling’ or ‘adaptive’ schedules. The default value is 0.0 as eta0 is not used by the default schedule ‘optimal’.
  
      @defaultValue `0`
     */
    eta0?: number;
    /**
      The exponent for inverse scaling learning rate [default 0.5].
  
      @defaultValue `0.5`
     */
    power_t?: number;
    /**
      When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See the Glossary.
  
      Repeatedly calling fit or partial_fit when warm_start is True can result in a different solution than when calling fit a single time because of the way the data is shuffled. If a dynamic learning rate is used, the learning rate is adapted depending on the number of samples already seen. Calling fit resets this counter, while partial_fit  will result in increasing the existing counter.
  
      @defaultValue `false`
     */
    warm_start?: boolean;
    /**
      When set to True, computes the averaged SGD weights and stores the result in the coef_ attribute. If set to an int greater than 1, averaging will begin once the total number of samples seen reaches average. So average=10 will begin averaging after seeing 10 samples.
  
      @defaultValue `false`
     */
    average?: boolean | number;
}
export interface SGDOneClassSVMDecisionFunctionOptions {
    /**
      Testing data.
     */
    X?: ArrayLike | SparseMatrix;
}
export interface SGDOneClassSVMDensifyOptions {
}
export interface SGDOneClassSVMFitOptions {
    /**
      Training data.
     */
    X?: ArrayLike | SparseMatrix;
    /**
      Not used, present for API consistency by convention.
     */
    y?: any;
    /**
      The initial coefficients to warm-start the optimization.
     */
    coef_init?: any;
    /**
      The initial offset to warm-start the optimization.
     */
    offset_init?: any;
    /**
      Weights applied to individual samples. If not provided, uniform weights are assumed. These weights will be multiplied with class_weight (passed through the constructor) if class_weight is specified.
     */
    sample_weight?: ArrayLike;
}
export interface SGDOneClassSVMFitPredictOptions {
    /**
      The input samples.
     */
    X?: ArrayLike | SparseMatrix[];
    /**
      Not used, present for API consistency by convention.
     */
    y?: any;
}
export interface SGDOneClassSVMPartialFitOptions {
    /**
      Subset of the training data.
     */
    X?: ArrayLike | SparseMatrix;
    /**
      Not used, present for API consistency by convention.
     */
    y?: any;
    /**
      Weights applied to individual samples. If not provided, uniform weights are assumed.
     */
    sample_weight?: ArrayLike;
}
export interface SGDOneClassSVMPredictOptions {
    /**
      Testing data.
     */
    X?: ArrayLike | SparseMatrix;
}
export interface SGDOneClassSVMScoreSamplesOptions {
    /**
      Testing data.
     */
    X?: ArrayLike | SparseMatrix;
}
export interface SGDOneClassSVMSparsifyOptions {
}
//# sourceMappingURL=SGDOneClassSVM.d.ts.map