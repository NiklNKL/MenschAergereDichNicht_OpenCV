import { PythonBridge, NDArray, ArrayLike, SparseMatrix } from '@/sklearn/types';
/**
  Lasso model fit with Least Angle Regression a.k.a. Lars.

  It is a Linear Model trained with an L1 prior as regularizer.

  The optimization objective for Lasso is:

  @see https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLars.html
 */
export declare class LassoLars {
    id: string;
    opts: any;
    _py: PythonBridge;
    _isInitialized: boolean;
    _isDisposed: boolean;
    constructor(opts?: LassoLarsOptions);
    get py(): PythonBridge;
    set py(pythonBridge: PythonBridge);
    /**
      Initializes the underlying Python resources.
  
      This instance is not usable until the `Promise` returned by `init()` resolves.
     */
    init(py: PythonBridge): Promise<void>;
    /**
      Disposes of the underlying Python resources.
  
      Once `dispose()` is called, the instance is no longer usable.
     */
    dispose(): Promise<void>;
    /**
      Fit the model using X, y as training data.
     */
    fit(opts: LassoLarsFitOptions): Promise<any>;
    /**
      Predict using the linear model.
     */
    predict(opts: LassoLarsPredictOptions): Promise<any>;
    /**
      Return the coefficient of determination of the prediction.
  
      The coefficient of determination \(R^2\) is defined as \((1 - \frac{u}{v})\), where \(u\) is the residual sum of squares ((y_true - y_pred)** 2).sum() and \(v\) is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a \(R^2\) score of 0.0.
     */
    score(opts: LassoLarsScoreOptions): Promise<number>;
    /**
      Maximum of covariances (in absolute value) at each iteration. n_alphas is either max_iter, n_features or the number of nodes in the path with alpha >= alpha_min, whichever is smaller. If this is a list of array-like, the length of the outer list is n_targets.
     */
    get alphas_(): Promise<ArrayLike>;
    /**
      Indices of active variables at the end of the path. If this is a list of list, the length of the outer list is n_targets.
     */
    get active_(): Promise<any>;
    /**
      If a list is passed it’s expected to be one of n_targets such arrays. The varying values of the coefficients along the path. It is not present if the fit_path parameter is False. If this is a list of array-like, the length of the outer list is n_targets.
     */
    get coef_path_(): Promise<ArrayLike[]>;
    /**
      Parameter vector (w in the formulation formula).
     */
    get coef_(): Promise<ArrayLike>;
    /**
      Independent term in decision function.
     */
    get intercept_(): Promise<number | ArrayLike>;
    /**
      The number of iterations taken by lars_path to find the grid of alphas for each target.
     */
    get n_iter_(): Promise<ArrayLike | number>;
    /**
      Number of features seen during fit.
     */
    get n_features_in_(): Promise<number>;
    /**
      Names of features seen during fit. Defined only when X has feature names that are all strings.
     */
    get feature_names_in_(): Promise<NDArray>;
}
export interface LassoLarsOptions {
    /**
      Constant that multiplies the penalty term. Defaults to 1.0. alpha = 0 is equivalent to an ordinary least square, solved by LinearRegression. For numerical reasons, using alpha = 0 with the LassoLars object is not advised and you should prefer the LinearRegression object.
  
      @defaultValue `1`
     */
    alpha?: number;
    /**
      Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered).
  
      @defaultValue `true`
     */
    fit_intercept?: boolean;
    /**
      Sets the verbosity amount.
  
      @defaultValue `false`
     */
    verbose?: boolean | number;
    /**
      This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use StandardScaler before calling fit on an estimator with normalize=False.
  
      @defaultValue `false`
     */
    normalize?: boolean;
    /**
      Whether to use a precomputed Gram matrix to speed up calculations. If set to 'auto' let us decide. The Gram matrix can also be passed as argument.
  
      @defaultValue `'auto'`
     */
    precompute?: boolean | 'auto' | ArrayLike;
    /**
      Maximum number of iterations to perform.
  
      @defaultValue `500`
     */
    max_iter?: number;
    /**
      The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the tol parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization.
     */
    eps?: number;
    /**
      If True, X will be copied; else, it may be overwritten.
  
      @defaultValue `true`
     */
    copy_X?: boolean;
    /**
      If True the full path is stored in the coef_path_ attribute. If you compute the solution for a large problem or many targets, setting fit_path to False will lead to a speedup, especially with a small alpha.
  
      @defaultValue `true`
     */
    fit_path?: boolean;
    /**
      Restrict coefficients to be >= 0. Be aware that you might want to remove fit_intercept which is set True by default. Under the positive restriction the model coefficients will not converge to the ordinary-least-squares solution for small values of alpha. Only coefficients up to the smallest alpha value (alphas_[alphas_ > 0.].min() when fit_path=True) reached by the stepwise Lars-Lasso algorithm are typically in congruence with the solution of the coordinate descent Lasso estimator.
  
      @defaultValue `false`
     */
    positive?: boolean;
    /**
      Upper bound on a uniform noise parameter to be added to the y values, to satisfy the model’s assumption of one-at-a-time computations. Might help with stability.
     */
    jitter?: number;
    /**
      Determines random number generation for jittering. Pass an int for reproducible output across multiple function calls. See Glossary. Ignored if jitter is None.
     */
    random_state?: number;
}
export interface LassoLarsFitOptions {
    /**
      Training data.
     */
    X?: ArrayLike[];
    /**
      Target values.
     */
    y?: ArrayLike;
    /**
      Xy = np.dot(X.T, y) that can be precomputed. It is useful only when the Gram matrix is precomputed.
     */
    Xy?: ArrayLike;
}
export interface LassoLarsPredictOptions {
    /**
      Samples.
     */
    X?: ArrayLike | SparseMatrix;
}
export interface LassoLarsScoreOptions {
    /**
      Test samples. For some estimators this may be a precomputed kernel matrix or a list of generic objects instead with shape (n_samples, n_samples_fitted), where n_samples_fitted is the number of samples used in the fitting for the estimator.
     */
    X?: ArrayLike[];
    /**
      True values for X.
     */
    y?: ArrayLike;
    /**
      Sample weights.
     */
    sample_weight?: ArrayLike;
}
//# sourceMappingURL=LassoLars.d.ts.map