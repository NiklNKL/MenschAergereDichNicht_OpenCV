import { PythonBridge, NDArray, ArrayLike, SparseMatrix } from '@/sklearn/types';
/**
  Epsilon-Support Vector Regression.

  The free parameters in the model are C and epsilon.

  The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to datasets with more than a couple of 10000 samples. For large datasets consider using LinearSVR or SGDRegressor instead, possibly after a Nystroem transformer or other Kernel Approximation.

  @see https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html
 */
export declare class SVR {
    id: string;
    opts: any;
    _py: PythonBridge;
    _isInitialized: boolean;
    _isDisposed: boolean;
    constructor(opts?: SVROptions);
    get py(): PythonBridge;
    set py(pythonBridge: PythonBridge);
    /**
      Initializes the underlying Python resources.
  
      This instance is not usable until the `Promise` returned by `init()` resolves.
     */
    init(py: PythonBridge): Promise<void>;
    /**
      Disposes of the underlying Python resources.
  
      Once `dispose()` is called, the instance is no longer usable.
     */
    dispose(): Promise<void>;
    /**
      Fit the SVM model according to the given training data.
     */
    fit(opts: SVRFitOptions): Promise<any>;
    /**
      Perform regression on samples in X.
  
      For an one-class model, +1 (inlier) or -1 (outlier) is returned.
     */
    predict(opts: SVRPredictOptions): Promise<NDArray>;
    /**
      Return the coefficient of determination of the prediction.
  
      The coefficient of determination \(R^2\) is defined as \((1 - \frac{u}{v})\), where \(u\) is the residual sum of squares ((y_true - y_pred)** 2).sum() and \(v\) is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a \(R^2\) score of 0.0.
     */
    score(opts: SVRScoreOptions): Promise<number>;
    /**
      Multipliers of parameter C for each class. Computed based on the class_weight parameter.
     */
    get class_weight_(): Promise<NDArray>;
    /**
      Coefficients of the support vector in the decision function.
     */
    get dual_coef_(): Promise<NDArray[]>;
    /**
      0 if correctly fitted, 1 otherwise (will raise warning)
     */
    get fit_status_(): Promise<number>;
    /**
      Constants in decision function.
     */
    get intercept_(): Promise<NDArray>;
    /**
      Number of features seen during fit.
     */
    get n_features_in_(): Promise<number>;
    /**
      Names of features seen during fit. Defined only when X has feature names that are all strings.
     */
    get feature_names_in_(): Promise<NDArray>;
    /**
      Number of iterations run by the optimization routine to fit the model.
     */
    get n_iter_(): Promise<number>;
    /**
      Array dimensions of training vector X.
     */
    get shape_fit_(): Promise<any[]>;
    /**
      Indices of support vectors.
     */
    get support_(): Promise<NDArray>;
    /**
      Support vectors.
     */
    get support_vectors_(): Promise<NDArray[]>;
}
export interface SVROptions {
    /**
      Specifies the kernel type to be used in the algorithm. If none is given, ‘rbf’ will be used. If a callable is given it is used to precompute the kernel matrix.
  
      @defaultValue `'rbf'`
     */
    kernel?: 'linear' | 'poly' | 'rbf' | 'sigmoid' | 'precomputed';
    /**
      Degree of the polynomial kernel function (‘poly’). Must be non-negative. Ignored by all other kernels.
  
      @defaultValue `3`
     */
    degree?: number;
    /**
      Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’.
  
      @defaultValue `'scale'`
     */
    gamma?: 'scale' | 'auto' | number;
    /**
      Independent term in kernel function. It is only significant in ‘poly’ and ‘sigmoid’.
  
      @defaultValue `0`
     */
    coef0?: number;
    /**
      Tolerance for stopping criterion.
  
      @defaultValue `0.001`
     */
    tol?: number;
    /**
      Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. The penalty is a squared l2 penalty.
  
      @defaultValue `1`
     */
    C?: number;
    /**
      Epsilon in the epsilon-SVR model. It specifies the epsilon-tube within which no penalty is associated in the training loss function with points predicted within a distance epsilon from the actual value. Must be non-negative.
  
      @defaultValue `0.1`
     */
    epsilon?: number;
    /**
      Whether to use the shrinking heuristic. See the User Guide.
  
      @defaultValue `true`
     */
    shrinking?: boolean;
    /**
      Specify the size of the kernel cache (in MB).
  
      @defaultValue `200`
     */
    cache_size?: number;
    /**
      Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in libsvm that, if enabled, may not work properly in a multithreaded context.
  
      @defaultValue `false`
     */
    verbose?: boolean;
    /**
      Hard limit on iterations within solver, or -1 for no limit.
  
      @defaultValue `-1`
     */
    max_iter?: number;
}
export interface SVRFitOptions {
    /**
      Training vectors, where n_samples is the number of samples and n_features is the number of features. For kernel=”precomputed”, the expected shape of X is (n_samples, n_samples).
     */
    X?: ArrayLike | SparseMatrix[];
    /**
      Target values (class labels in classification, real numbers in regression).
     */
    y?: ArrayLike;
    /**
      Per-sample weights. Rescale C per sample. Higher weights force the classifier to put more emphasis on these points.
     */
    sample_weight?: ArrayLike;
}
export interface SVRPredictOptions {
    /**
      For kernel=”precomputed”, the expected shape of X is (n_samples_test, n_samples_train).
     */
    X?: ArrayLike | SparseMatrix[];
}
export interface SVRScoreOptions {
    /**
      Test samples. For some estimators this may be a precomputed kernel matrix or a list of generic objects instead with shape (n_samples, n_samples_fitted), where n_samples_fitted is the number of samples used in the fitting for the estimator.
     */
    X?: ArrayLike[];
    /**
      True values for X.
     */
    y?: ArrayLike;
    /**
      Sample weights.
     */
    sample_weight?: ArrayLike;
}
//# sourceMappingURL=SVR.d.ts.map