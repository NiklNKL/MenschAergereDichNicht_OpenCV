import { PythonBridge, NDArray, ArrayLike, SparseMatrix } from '@/sklearn/types';
/**
  An extremely randomized tree regressor.

  Extra-trees differ from classic decision trees in the way they are built. When looking for the best split to separate the samples of a node into two groups, random splits are drawn for each of the max_features randomly selected features and the best split among those is chosen. When max_features is set 1, this amounts to building a totally random decision tree.

  Warning: Extra-trees should only be used within ensemble methods.

  @see https://scikit-learn.org/stable/modules/generated/sklearn.tree.ExtraTreeRegressor.html
 */
export declare class ExtraTreeRegressor {
    id: string;
    opts: any;
    _py: PythonBridge;
    _isInitialized: boolean;
    _isDisposed: boolean;
    constructor(opts?: ExtraTreeRegressorOptions);
    get py(): PythonBridge;
    set py(pythonBridge: PythonBridge);
    /**
      Initializes the underlying Python resources.
  
      This instance is not usable until the `Promise` returned by `init()` resolves.
     */
    init(py: PythonBridge): Promise<void>;
    /**
      Disposes of the underlying Python resources.
  
      Once `dispose()` is called, the instance is no longer usable.
     */
    dispose(): Promise<void>;
    /**
      Return the index of the leaf that each sample is predicted as.
     */
    apply(opts: ExtraTreeRegressorApplyOptions): Promise<ArrayLike>;
    /**
      Compute the pruning path during Minimal Cost-Complexity Pruning.
  
      See Minimal Cost-Complexity Pruning for details on the pruning process.
     */
    cost_complexity_pruning_path(opts: ExtraTreeRegressorCostComplexityPruningPathOptions): Promise<any>;
    /**
      Return the decision path in the tree.
     */
    decision_path(opts: ExtraTreeRegressorDecisionPathOptions): Promise<SparseMatrix[]>;
    /**
      Build a decision tree regressor from the training set (X, y).
     */
    fit(opts: ExtraTreeRegressorFitOptions): Promise<any>;
    /**
      Return the depth of the decision tree.
  
      The depth of a tree is the maximum distance between the root and any leaf.
     */
    get_depth(opts: ExtraTreeRegressorGetDepthOptions): Promise<any>;
    /**
      Return the number of leaves of the decision tree.
     */
    get_n_leaves(opts: ExtraTreeRegressorGetNLeavesOptions): Promise<any>;
    /**
      Predict class or regression value for X.
  
      For a classification model, the predicted class for each sample in X is returned. For a regression model, the predicted value based on X is returned.
     */
    predict(opts: ExtraTreeRegressorPredictOptions): Promise<ArrayLike>;
    /**
      Return the coefficient of determination of the prediction.
  
      The coefficient of determination \(R^2\) is defined as \((1 - \frac{u}{v})\), where \(u\) is the residual sum of squares ((y_true - y_pred)** 2).sum() and \(v\) is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a \(R^2\) score of 0.0.
     */
    score(opts: ExtraTreeRegressorScoreOptions): Promise<number>;
    /**
      The inferred value of max_features.
     */
    get max_features_(): Promise<number>;
    /**
      Number of features seen during fit.
     */
    get n_features_in_(): Promise<number>;
    /**
      Names of features seen during fit. Defined only when X has feature names that are all strings.
     */
    get feature_names_in_(): Promise<NDArray>;
    /**
      The number of outputs when fit is performed.
     */
    get n_outputs_(): Promise<number>;
    /**
      The underlying Tree object. Please refer to help(sklearn.tree._tree.Tree) for attributes of Tree object and Understanding the decision tree structure for basic usage of these attributes.
     */
    get tree_(): Promise<any>;
}
export interface ExtraTreeRegressorOptions {
    /**
      The function to measure the quality of a split. Supported criteria are “squared_error” for the mean squared error, which is equal to variance reduction as feature selection criterion and minimizes the L2 loss using the mean of each terminal node, “friedman_mse”, which uses mean squared error with Friedman’s improvement score for potential splits, “absolute_error” for the mean absolute error, which minimizes the L1 loss using the median of each terminal node, and “poisson” which uses reduction in Poisson deviance to find splits.
  
      @defaultValue `'squared_error'`
     */
    criterion?: 'squared_error' | 'friedman_mse' | 'absolute_error' | 'poisson';
    /**
      The strategy used to choose the split at each node. Supported strategies are “best” to choose the best split and “random” to choose the best random split.
  
      @defaultValue `'random'`
     */
    splitter?: 'random' | 'best';
    /**
      The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.
     */
    max_depth?: number;
    /**
      The minimum number of samples required to split an internal node:
  
      @defaultValue `2`
     */
    min_samples_split?: number;
    /**
      The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches.  This may have the effect of smoothing the model, especially in regression.
  
      @defaultValue `1`
     */
    min_samples_leaf?: number;
    /**
      The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.
  
      @defaultValue `0`
     */
    min_weight_fraction_leaf?: number;
    /**
      The number of features to consider when looking for the best split:
  
      @defaultValue `1`
     */
    max_features?: number | 'sqrt';
    /**
      Used to pick randomly the max_features used at each split. See Glossary for details.
     */
    random_state?: number;
    /**
      A node will be split if this split induces a decrease of the impurity greater than or equal to this value.
  
      The weighted impurity decrease equation is the following:
  
      @defaultValue `0`
     */
    min_impurity_decrease?: number;
    /**
      Grow a tree with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.
     */
    max_leaf_nodes?: number;
    /**
      Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen. By default, no pruning is performed. See Minimal Cost-Complexity Pruning for details.
  
      @defaultValue `0`
     */
    ccp_alpha?: any;
}
export interface ExtraTreeRegressorApplyOptions {
    /**
      The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix.
     */
    X?: ArrayLike | SparseMatrix[];
    /**
      Allow to bypass several input checking. Don’t use this parameter unless you know what you’re doing.
  
      @defaultValue `true`
     */
    check_input?: boolean;
}
export interface ExtraTreeRegressorCostComplexityPruningPathOptions {
    /**
      The training input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csc_matrix.
     */
    X?: ArrayLike | SparseMatrix[];
    /**
      The target values (class labels) as integers or strings.
     */
    y?: ArrayLike;
    /**
      Sample weights. If None, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node. Splits are also ignored if they would result in any single class carrying a negative weight in either child node.
     */
    sample_weight?: ArrayLike;
}
export interface ExtraTreeRegressorDecisionPathOptions {
    /**
      The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix.
     */
    X?: ArrayLike | SparseMatrix[];
    /**
      Allow to bypass several input checking. Don’t use this parameter unless you know what you’re doing.
  
      @defaultValue `true`
     */
    check_input?: boolean;
}
export interface ExtraTreeRegressorFitOptions {
    /**
      The training input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csc_matrix.
     */
    X?: ArrayLike | SparseMatrix[];
    /**
      The target values (real numbers). Use dtype=np.float64 and order='C' for maximum efficiency.
     */
    y?: ArrayLike;
    /**
      Sample weights. If None, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node.
     */
    sample_weight?: ArrayLike;
    /**
      Allow to bypass several input checking. Don’t use this parameter unless you know what you’re doing.
  
      @defaultValue `true`
     */
    check_input?: boolean;
}
export interface ExtraTreeRegressorGetDepthOptions {
}
export interface ExtraTreeRegressorGetNLeavesOptions {
}
export interface ExtraTreeRegressorPredictOptions {
    /**
      The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix.
     */
    X?: ArrayLike | SparseMatrix[];
    /**
      Allow to bypass several input checking. Don’t use this parameter unless you know what you’re doing.
  
      @defaultValue `true`
     */
    check_input?: boolean;
}
export interface ExtraTreeRegressorScoreOptions {
    /**
      Test samples. For some estimators this may be a precomputed kernel matrix or a list of generic objects instead with shape (n_samples, n_samples_fitted), where n_samples_fitted is the number of samples used in the fitting for the estimator.
     */
    X?: ArrayLike[];
    /**
      True values for X.
     */
    y?: ArrayLike;
    /**
      Sample weights.
     */
    sample_weight?: ArrayLike;
}
//# sourceMappingURL=ExtraTreeRegressor.d.ts.map