import { PythonBridge, NDArray, ArrayLike, SparseMatrix } from '@/sklearn/types';
/**
  Isolation Forest Algorithm.

  Return the anomaly score of each sample using the IsolationForest algorithm

  The IsolationForest ‘isolates’ observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.

  Since recursive partitioning can be represented by a tree structure, the number of splittings required to isolate a sample is equivalent to the path length from the root node to the terminating node.

  This path length, averaged over a forest of such random trees, is a measure of normality and our decision function.

  Random partitioning produces noticeably shorter paths for anomalies. Hence, when a forest of random trees collectively produce shorter path lengths for particular samples, they are highly likely to be anomalies.

  @see https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html
 */
export declare class IsolationForest {
    id: string;
    opts: any;
    _py: PythonBridge;
    _isInitialized: boolean;
    _isDisposed: boolean;
    constructor(opts?: IsolationForestOptions);
    get py(): PythonBridge;
    set py(pythonBridge: PythonBridge);
    /**
      Initializes the underlying Python resources.
  
      This instance is not usable until the `Promise` returned by `init()` resolves.
     */
    init(py: PythonBridge): Promise<void>;
    /**
      Disposes of the underlying Python resources.
  
      Once `dispose()` is called, the instance is no longer usable.
     */
    dispose(): Promise<void>;
    /**
      Average anomaly score of X of the base classifiers.
  
      The anomaly score of an input sample is computed as the mean anomaly score of the trees in the forest.
  
      The measure of normality of an observation given a tree is the depth of the leaf containing this observation, which is equivalent to the number of splittings required to isolate this point. In case of several observations n_left in the leaf, the average path length of a n_left samples isolation tree is added.
     */
    decision_function(opts: IsolationForestDecisionFunctionOptions): Promise<NDArray>;
    /**
      Fit estimator.
     */
    fit(opts: IsolationForestFitOptions): Promise<any>;
    /**
      Perform fit on X and returns labels for X.
  
      Returns -1 for outliers and 1 for inliers.
     */
    fit_predict(opts: IsolationForestFitPredictOptions): Promise<NDArray>;
    /**
      Predict if a particular sample is an outlier or not.
     */
    predict(opts: IsolationForestPredictOptions): Promise<NDArray>;
    /**
      Opposite of the anomaly score defined in the original paper.
  
      The anomaly score of an input sample is computed as the mean anomaly score of the trees in the forest.
  
      The measure of normality of an observation given a tree is the depth of the leaf containing this observation, which is equivalent to the number of splittings required to isolate this point. In case of several observations n_left in the leaf, the average path length of a n_left samples isolation tree is added.
     */
    score_samples(opts: IsolationForestScoreSamplesOptions): Promise<NDArray>;
    /**
      The child estimator template used to create the collection of fitted sub-estimators.
     */
    get estimator_(): Promise<any>;
    /**
      The collection of fitted sub-estimators.
     */
    get estimators_(): Promise<any>;
    /**
      The subset of drawn features for each base estimator.
     */
    get estimators_features_(): Promise<any>;
    /**
      The actual number of samples.
     */
    get max_samples_(): Promise<number>;
    /**
      Offset used to define the decision function from the raw scores. We have the relation: decision_function = score_samples - offset_. offset_ is defined as follows. When the contamination parameter is set to “auto”, the offset is equal to -0.5 as the scores of inliers are close to 0 and the scores of outliers are close to -1. When a contamination parameter different than “auto” is provided, the offset is defined in such a way we obtain the expected number of outliers (samples with decision function < 0) in training.
     */
    get offset_(): Promise<number>;
    /**
      Number of features seen during fit.
     */
    get n_features_in_(): Promise<number>;
    /**
      Names of features seen during fit. Defined only when X has feature names that are all strings.
     */
    get feature_names_in_(): Promise<NDArray>;
}
export interface IsolationForestOptions {
    /**
      The number of base estimators in the ensemble.
  
      @defaultValue `100`
     */
    n_estimators?: number;
    /**
      If int, then draw max_samples samples.
  
      @defaultValue `'auto'`
     */
    max_samples?: 'auto' | number | number;
    /**
      The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the scores of the samples.
  
      @defaultValue `'auto'`
     */
    contamination?: 'auto' | number;
    /**
      The number of features to draw from X to train each base estimator.
  
      @defaultValue `1`
     */
    max_features?: number;
    /**
      If True, individual trees are fit on random subsets of the training data sampled with replacement. If False, sampling without replacement is performed.
  
      @defaultValue `false`
     */
    bootstrap?: boolean;
    /**
      The number of jobs to run in parallel for both fit and predict. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details.
     */
    n_jobs?: number;
    /**
      Controls the pseudo-randomness of the selection of the feature and split values for each branching step and each tree in the forest.
  
      Pass an int for reproducible results across multiple function calls. See Glossary.
     */
    random_state?: number;
    /**
      Controls the verbosity of the tree building process.
  
      @defaultValue `0`
     */
    verbose?: number;
    /**
      When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest. See the Glossary.
  
      @defaultValue `false`
     */
    warm_start?: boolean;
}
export interface IsolationForestDecisionFunctionOptions {
    /**
      The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix.
     */
    X?: ArrayLike | SparseMatrix[];
}
export interface IsolationForestFitOptions {
    /**
      The input samples. Use dtype=np.float32 for maximum efficiency. Sparse matrices are also supported, use sparse csc_matrix for maximum efficiency.
     */
    X?: ArrayLike | SparseMatrix[];
    /**
      Not used, present for API consistency by convention.
     */
    y?: any;
    /**
      Sample weights. If None, then samples are equally weighted.
     */
    sample_weight?: ArrayLike;
}
export interface IsolationForestFitPredictOptions {
    /**
      The input samples.
     */
    X?: ArrayLike | SparseMatrix[];
    /**
      Not used, present for API consistency by convention.
     */
    y?: any;
}
export interface IsolationForestPredictOptions {
    /**
      The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix.
     */
    X?: ArrayLike | SparseMatrix[];
}
export interface IsolationForestScoreSamplesOptions {
    /**
      The input samples.
     */
    X?: ArrayLike | SparseMatrix[];
}
//# sourceMappingURL=IsolationForest.d.ts.map